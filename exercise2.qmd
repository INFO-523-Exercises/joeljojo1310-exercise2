------------------------------------------------------------------------

---
title: "Exercise 2"
format: html
editor: visual
author: Joel Jojo
---

## Summary

Topics included in this exercise are data submission, package installation, data loading, cleaning, scaling, correlations, dimensionality reduction, sampling, and handling text datasets.

Submission: How to submit data for analysis.

Installing Required Packages: Installing and managing R packages.

Loading Data: Importing datasets into R.

Data Cleaning: Removing inconsistencies and errors in data.

Scaling and Normalization: Standardizing data for analysis.

Variable Correlations and Dimensionality Reduction: Analyzing feature relationships and reducing dimensions.

Sampling: Methods for selecting representative data subsets.

Handling Text Datasets: Techniques for working with text-based data.

## Code

The below code installs the "pacman" package which helps in packet installation and management in R.

```{r}
# First run this
install.packages("pacman")
```

```{r}
library(pacman)

p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL,
       sampling,# Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr,
       tm) 
```

#### LOADING DATA

**In the below section we practice loading data from a .csv or .txt file making them available for analysis and other purposes.**

The below code reads the x.csv file into "data" data frame using the read_csv() function.

data \|\> glimpse() line passes the data frame into the glimpse() function which then displays a concise summary of the data frame's structure.

```{r}
data <- read_csv(here("data", "x.csv"))

data |> glimpse()
```

The below code reads a tab delimited file from the "data" directory using read_delim() function. Then the glimpse function produces a summary of the data frame.

```{r}
data <- read_delim(here("data", "x.tsv"))

data |> glimpse()
```

Unable to recreate **Importing data from MySQL database** section since the connection to MySQL isn't getting established.\
\

#### DATA CLEANING

**In the below section we will be experimenting with data cleaning methods wherein we identify and correct errors, inconsistencies and missing values.**

\
The below code loads the wide.txt file into the "wide" data frame and using the delimiter " " it skips the first row of data. The col_names parameter provides custom column names to the data frame.

```{r}
wide <- read_delim(here("data", "wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

The below code converts the data frame from a wide format to a long format.

cols = c(Math, English) line helps pivot the columns (Math, English) from a wide to a long format.

names_to = "Subject" line specifies that there will be a new column by the name "Subject" which will have the above columns as values.

values_to = "Grade" line specifies that there will be a new column by the name "Grade" which will have the grades corresponding to each subject.

```{r}
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

In the below code we change the data frame from a long format to a wide format.

names_from = Subject argument specifies that the values in "Subject" column can be used as columns in the new data frame.

values_from = Grade argument specifies that the values in the "Grade" column should be used as the values for the new columns in the data frame.

```{r}
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

The below code uses the separate() function to separate the column "Degree_Year" into two new columns "Degree" and "Year".

The values in the column will be separated using the underscore (\_) as the separator.

```{r}
clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean
```

```{r}
install.packages("lubridate")
library(lubridate)
```

The below code enters date in different formats into mixed.dates data frame.

It is then put into the ymd() function which then converts all the dates to a year-month-date format

```{r}
mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day format
clean.dates

```

The below code uses various functions of the lubridate package to clean and extract date information.

The wday() function extracts the numeric representation of the day of the week(1 for sunday, 2 for monday etc.). If the same function has the label argument set to TRUE then it provides the label for the day of the week.\
year() function extracts the year information from the date.

month() function extracts the month information from the date.

```{r}
data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

The below code contains assigns the Asia/Shanghai equivalent value of "20190203 03:00:03" to date.time

```{r}
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

The below code changes the date.time value using the with_tz() function to America/Phoenix time.

```{r}
with_tz(date.time, tz="America/Phoenix")
```

The below code utilizes the force_tz() function to forcefully change the date.time value from Phoenix time to Turkey time

```{r}
force_tz(date.time, "Turkey")
```

The below code returns a list of timezones which can be used further while working on timezones.

```{r}
OlsonNames()

```

```{r}
library(dplyr)
library(stringr)
library(readr)
```

The variable uci.repo stores a character string representing the base URL of the UCI Machine Learning Repository. The base URL hosts different ML datasets.

The "dataset" variable stores a dataset, specifically "audiology" from the ML repository.

```{r}
uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

The below code uses the str_c() function to concatenate three strings together. This helps achieve complete access to the data and associated variable names for the specified dataset in the UCI Machine Learning directory.

```{r}
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

This line of code reads the first row of data without considering them as columns (col_names= FALSE) and the missing values "?" are replaced with NA. The url() function helps create a connection to the specified URL.

```{r}
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

The below code returns the dimensions(number of rows, number of columns) of the "data" data frame using the dim() function.

```{r}
dim(data)
```

The below code reads the text content from the URL provided as an argument and stores it in the "lines" variable of which a few lines are displayed using the head() function.

```{r}
lines <- read_lines(url(namesF))

lines |> head()
```

The below code reads the lines 67 through 135 and displays it through the "names" variable.

```{r}
names <- lines[67:135]
names
```

This line of code uses the str_split_fixed() function to split the names vector whenever it encounters a ":" character and the 2 argument indicates that it should split each element imto at most two parts.

```{r}
names <- str_split_fixed(names, ":", 2) #split on regular expression pattern ":", this function returns a matrix
names
```

The below code selects the first column and it's set of values and displays it

```{r}
names <- names[,1]
names
```

```{r}
names <-str_trim(names) |> str_replace_all("\\(|\\)", "") # we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.
names
```

The below code sets the names to the columns from 1 to 69. It takes the first 69 columns and assigns the corresponding values from the "names" vector.

This is a common step in data analysis to ensure that the columns of the data frame have meaningful names associated with the variables they represent.

```{r}
colnames(data)[1:69] <- names
data
```

The below line of code assigns the column name "id" to column 70 and name "class" to column 71.

```{r}
colnames(data)[70:71] <- c("id", "class")
data
```

The below code helps filter out the rows with one or more missing values using the filter() and !complete.cases() function. These rows can further be handled through removal, imputation or other methods.

```{r}
library(dplyr)

missing.value.rows <- data |>
  filter(!complete.cases(data))
missing.value.rows
```

The below line of code creates a new column with the help of the mutate() function with the name "na_count". This new column calculates the sum of NA values in each row. This helps in understanding data quality in each row.

```{r}
data <- data %>%
  mutate(na_count = rowSums(is.na(data)))
data
```

The below code calculates the sum of NA values in each column of the "data" data frame through the -sum(is,na(.)) part of code. The summarize() function summarizes the output and creates a new data frame.

The pivot_longer() function changes the output from a wide to a long format and columns "column_names" and "na_count" are added to the output. Finally the arrange() function arranges data in the ascending value of NA count.

```{r}
data |>
  summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

The below code does the same as the above one expect that row number 8 is removed from the data frame using the select() function.

```{r}
data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
data.bser.removed
```

The data data frame in the below code will retain only the columns that do not have "bser" in their names, effectively removing columns with names containing that term.

```{r}
data <- data %>%
  select(-matches("bser"))
```

The mistaken variable in the code is a numeric vector that contains the values 2, 3, 4, and "?". However, because one of the values is a character (the "?"), R will coerce all the values in the vector to characters, making it a character vector.

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

The mistaken character vector is fixed by converting it into an integer vector using the parse_integer() function from the readr package. Additionally, it specifies that the "?" character should be treated as NA (missing value).

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

This code should load the "algae" dataset and display the 48th row of data from the "algae" dataset.

```{r}
install.packages("DMwR2")
library(DMwR2)
data(algae, package = "DMwR2")
algae[48,]
```

The below code initializes a ggplot2 using the "algae" dataset using the mxPH variable as the sample data. The geom_qq_band() function adds a shaded region representing the confidence intervals. The stat_qq_point() function adds points to the qq plots to represent the observed quantiles of the mxPH variables. This plot allows to visually assess wether the data follows a normal distribution. The reference line helps you identify deviations from normality.

```{r}
# plot a QQ plot of mxPH
install.packages("car")
library(car)
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

The below line of code imputes any missing values from the 48th row with the mean of the mxPH variable.

```{r}
algae <- algae |>
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

The below code plots the same QQ plot as before but with the variable "Chla"

```{r}
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

```{r}
median(algae$Chla, na.rm = TRUE)
```

```{r}
mean(algae$Chla, na.rm = TRUE)
```

```{r}
algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

```{r}
algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric |> correlate() |> plot()
```

```{r}
cor_matrix
```

```{r}
algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. 

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

```{r}
m |> 
  summary()
```

```{r}
m |> 
  summary() |> 
  tidy()
```

```{r}
algae$PO4
```

```{r}
algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
```

```{r}
res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

```{r}
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

```{r}
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
#if x is not NA, return 42.897+1.293*x 
```

```{r}
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

```{r}
algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


data(algae, package="DMwR2") #get data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #use the median of k most similar samples
```

```{r}
getAnywhere(knnImputation())
```

```{r}
library(dplyr)
library(palmerpenguins)
```

```{r}
data(penguins)
```

```{r}
# select only numeric columns
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# normalize numeric columns
penguins_norm <- scale(penguins_numeric)

# convert back to data frame and add species column
peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.
```

```{r}
summary(penguins)
```

```{r}
summary(peng.norm)
```

```{r}
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
```

```{r}
max
```

```{r}
min

```

```{r}
# min-max normalization
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

```{r}
data(Boston, package="MASS")
summary(Boston$age)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") #create 5 bins and add new column newAge to Boston
summary(Boston$newAge)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") #add labels

summary(Boston$newAge)
```

```{r}

install.packages("Hmisc")
library(Hmisc)
Boston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins and add new column newAge to Boston

table(Boston$newAge)
```

```{r}
Boston$newAge <- factor(cut2(Boston$age, g = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

```{r}
hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 – 101.
```

```{r}
library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

```{r}
data <- c(10, 20, 30, 50, 100)
```

```{r}
(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters
```

```{r}
(decimalScale = data / (10^nDigits))
```

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

```{r}
(bin_means = apply(bins, 1, FUN = mean))
```

```{r}
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

```{r}
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

```{r}
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
seniors
```

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

```{r}
mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

```{r}
mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

```{r}
cbind(mod.3$data, fitted(mod.3))
```

```{r}
library(tidyr) # data manipulation
penguins_numeric |> 
  drop_na() |>
  correlate()
```

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
head(pca$scores) # pca result is a list, and the component scores are elements in the list
```

```{r}
penguins_na <- penguins |> 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

```{r}
install.packages("wavelets")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3) #with 8-element vector, 3 level is the max.
wt
```

```{r}
idwt(wt)
```

```{r}
xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

```{r}
idwt(xt)
```

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

```{r}
sample(age, 5)
```

```{r}
sample(age, 5, replace = TRUE)
```

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

```{r}

age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

```{r}


# read corpus
# Documents is a folder in current working directory, holding some of Hillary's emails
docs <- Corpus(DirSource("Documents"))
mode(docs)
```

```{r}
docs[[20]]


```

```{r}
docs <- docs |>
         tm_map(removePunctuation) |>
         tm_map(content_transformer(tolower)) |> #to lower case
         tm_map(removeNumbers) |>
         tm_map(removeWords, stopwords("en")) |> #stopwords, such as a, an.
         tm_map(stripWhitespace) |>
         tm_map(stemDocument) #e.g. computer -> comput
```

```{r}
install.packages("wordcloud")
install.packages("RColorBrewer")
library(wordcloud)
```

```{r}
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```
