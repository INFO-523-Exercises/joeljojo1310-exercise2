------------------------------------------------------------------------

---
title: "Exercise 2"
format: html
editor: visual
author: Joel Jojo
---

## Summary

Topics included in this exercise are data submission, package installation, data loading, cleaning, scaling, correlations, dimensionality reduction, sampling, and handling text datasets.

Submission: How to submit data for analysis.

Installing Required Packages: Installing and managing R packages.

Loading Data: Importing datasets into R.

Data Cleaning: Removing inconsistencies and errors in data.

Scaling and Normalization: Standardizing data for analysis.

Variable Correlations and Dimensionality Reduction: Analyzing feature relationships and reducing dimensions.

Sampling: Methods for selecting representative data subsets.

Handling Text Datasets: Techniques for working with text-based data.

## Code

The below code installs the "pacman" package which helps in packet installation and management in R.

```{r}
# First run this
install.packages("pacman")
```

```{r}
library(pacman)

p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL,
       sampling,# Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr,
       tm) 
```

#### LOADING DATA

**In the below section we practice loading data from a .csv or .txt file making them available for analysis and other purposes.**

The below code reads the x.csv file into "data" data frame using the read_csv() function.

data \|\> glimpse() line passes the data frame into the glimpse() function which then displays a concise summary of the data frame's structure.

```{r}
data <- read_csv(here("data", "x.csv"))

data |> glimpse()
```

The below code reads a tab delimited file from the "data" directory using read_delim() function. Then the glimpse function produces a summary of the data frame.

```{r}
data <- read_delim(here("data", "x.tsv"))

data |> glimpse()
```

Unable to recreate **Importing data from MySQL database** section since the connection to MySQL isn't getting established.\
\

#### DATA CLEANING

**In the below section we will be experimenting with data cleaning methods wherein we identify and correct errors, inconsistencies and missing values.**

\
The below code loads the wide.txt file into the "wide" data frame and using the delimiter " " it skips the first row of data. The col_names parameter provides custom column names to the data frame.

```{r}
wide <- read_delim(here("data", "wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

The below code converts the data frame from a wide format to a long format.

cols = c(Math, English) line helps pivot the columns (Math, English) from a wide to a long format.

names_to = "Subject" line specifies that there will be a new column by the name "Subject" which will have the above columns as values.

values_to = "Grade" line specifies that there will be a new column by the name "Grade" which will have the grades corresponding to each subject.

```{r}
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

In the below code we change the data frame from a long format to a wide format.

names_from = Subject argument specifies that the values in "Subject" column can be used as columns in the new data frame.

values_from = Grade argument specifies that the values in the "Grade" column should be used as the values for the new columns in the data frame.

```{r}
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

The below code uses the separate() function to separate the column "Degree_Year" into two new columns "Degree" and "Year".

The values in the column will be separated using the underscore (\_) as the separator.

```{r}
clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean
```

```{r}
install.packages("lubridate")
library(lubridate)
```

The below code enters date in different formats into mixed.dates data frame.

It is then put into the ymd() function which then converts all the dates to a year-month-date format

```{r}
mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day format
clean.dates

```

The below code uses various functions of the lubridate package to clean and extract date information.

The wday() function extracts the numeric representation of the day of the week(1 for sunday, 2 for monday etc.). If the same function has the label argument set to TRUE then it provides the label for the day of the week.\
year() function extracts the year information from the date.

month() function extracts the month information from the date.

```{r}
data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

The below code contains assigns the Asia/Shanghai equivalent value of "20190203 03:00:03" to date.time

```{r}
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

The below code changes the date.time value using the with_tz() function to America/Phoenix time.

```{r}
with_tz(date.time, tz="America/Phoenix")
```

The below code utilizes the force_tz() function to forcefully change the date.time value from Phoenix time to Turkey time

```{r}
force_tz(date.time, "Turkey")
```

The below code returns a list of timezones which can be used further while working on timezones.

```{r}
OlsonNames()

```

```{r}
library(dplyr)
library(stringr)
library(readr)
```

The variable uci.repo stores a character string representing the base URL of the UCI Machine Learning Repository. The base URL hosts different ML datasets.

The "dataset" variable stores a dataset, specifically "audiology" from the ML repository.

```{r}
uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

The below code uses the str_c() function to concatenate three strings together. This helps achieve complete access to the data and associated variable names for the specified dataset in the UCI Machine Learning directory.

```{r}
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

This line of code reads the first row of data without considering them as columns (col_names= FALSE) and the missing values "?" are replaced with NA. The url() function helps create a connection to the specified URL.

```{r}
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

The below code returns the dimensions(number of rows, number of columns) of the "data" data frame using the dim() function.

```{r}
dim(data)
```

The below code reads the text content from the URL provided as an argument and stores it in the "lines" variable of which a few lines are displayed using the head() function.

```{r}
lines <- read_lines(url(namesF))

lines |> head()
```

The below code reads the lines 67 through 135 and displays it through the "names" variable.

```{r}
names <- lines[67:135]
names
```

This line of code uses the str_split_fixed() function to split the names vector whenever it encounters a ":" character and the 2 argument indicates that it should split each element imto at most two parts.

```{r}
names <- str_split_fixed(names, ":", 2) #split on regular expression pattern ":", this function returns a matrix
names
```

The below code selects the first column and it's set of values and displays it

```{r}
names <- names[,1]
names
```

```{r}
names <-str_trim(names) |> str_replace_all("\\(|\\)", "") # we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.
names
```

The below code sets the names to the columns from 1 to 69. It takes the first 69 columns and assigns the corresponding values from the "names" vector.

This is a common step in data analysis to ensure that the columns of the data frame have meaningful names associated with the variables they represent.

```{r}
colnames(data)[1:69] <- names
data
```

The below line of code assigns the column name "id" to column 70 and name "class" to column 71.

```{r}
colnames(data)[70:71] <- c("id", "class")
data
```

The below code helps filter out the rows with one or more missing values using the filter() and !complete.cases() function. These rows can further be handled through removal, imputation or other methods.

```{r}
library(dplyr)

missing.value.rows <- data |>
  filter(!complete.cases(data))
missing.value.rows
```

The below line of code creates a new column with the help of the mutate() function with the name "na_count". This new column calculates the sum of NA values in each row. This helps in understanding data quality in each row.

```{r}
data <- data %>%
  mutate(na_count = rowSums(is.na(data)))
data
```

The below code calculates the sum of NA values in each column of the "data" data frame through the -sum(is,na(.)) part of code. The summarize() function summarizes the output and creates a new data frame.

The pivot_longer() function changes the output from a wide to a long format and columns "column_names" and "na_count" are added to the output. Finally the arrange() function arranges data in the ascending value of NA count.

```{r}
data |>
  summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

The below code does the same as the above one expect that row number 8 is removed from the data frame using the select() function.

```{r}
data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
data.bser.removed
```

The data data frame in the below code will retain only the columns that do not have "bser" in their names, effectively removing columns with names containing that term.

```{r}
data <- data %>%
  select(-matches("bser"))
```

The mistaken variable in the code is a numeric vector that contains the values 2, 3, 4, and "?". However, because one of the values is a character (the "?"), R will coerce all the values in the vector to characters, making it a character vector.

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

The mistaken character vector is fixed by converting it into an integer vector using the parse_integer() function from the readr package. Additionally, it specifies that the "?" character should be treated as NA (missing value).

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

This code should load the "algae" dataset and display the 48th row of data from the "algae" dataset.

```{r}
install.packages("DMwR2")
library(DMwR2)
data(algae, package = "DMwR2")
algae[48,]
```

The below code initializes a ggplot2 using the "algae" dataset using the mxPH variable as the sample data. The geom_qq_band() function adds a shaded region representing the confidence intervals. The stat_qq_point() function adds points to the qq plots to represent the observed quantiles of the mxPH variables. This plot allows to visually assess wether the data follows a normal distribution. The reference line helps you identify deviations from normality.

```{r}
# plot a QQ plot of mxPH
install.packages("car")
library(car)
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

The below line of code imputes any missing values from the 48th row with the mean of the mxPH variable.

```{r}
algae <- algae |>
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

The below code plots the same QQ plot as before but with the variable "Chla"

```{r}
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

The below code gives the median value of the "Chla" variable in the dataset, which provides a measure of the central tendency of the non-missing values in that variable.

```{r}
median(algae$Chla, na.rm = TRUE)
```

The below code provides the mean value of the "Chla" variable in the dataset, based on the non-missing values. It provides a measure of the central tendency of the data.

```{r}
mean(algae$Chla, na.rm = TRUE)
```

The below code removes missing values from Chla variable and replaces it with the median of the non missing values from the Chla column. This common technique for imputing missing data helps make the dataset more complete for analysis.

```{r}
algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

The below code assigns the columns 4 to 18 to the to the variable algae_numeric and then the NA values are removed using the drop_na() function. Finally a correlation matrix of the numeric variables in algae_numeric using correlate() function. This is finally plotted using the plot() function.

```{r}
algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric |> correlate() |> plot()
```

```{r}
cor_matrix
```

The below code filters out rows where the missing values percentage exceeds 20%. This is then used in a linear regression model where the dependant variable is PO4 and the independant variable is oPO4.

lm(formula = PO4 \~ oPO4, data = algae) is the linear regression model formula used.

```{r}
algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. 

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

The summary() function here provides a desciprtion of the linear regressoin model object, including coefficients, standard errors and other predictors that describes the model's fit.

```{r}
m |> 
  summary()
```

The tidy() function represents the linear regerssion object model in a tabular and organized manner, making it easier to work with the results of the regression model.

```{r}
m |> 
  summary() |> 
  tidy()
```

By using algae\$PO4, we retrieve and access the values of the "PO4" variable from the "algae" dataset.

```{r}
algae$PO4
```

Here, the PO4 variable is mutated with the help of the mutate function. The 28th row in the PO4 variable is replaced with a new value based on arithmetic operation on the oPO4 variable. This is a way to conditionally update a specific row in a data frame based on a row number.

```{r}
algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
```

the resid() function here calculates the residuals of the linear regression model m. Residuals represent the differences between the observed values and the values predicted by the model.

algae %\>% filter(row_number() != 28): it creates a reduced dataset without the 28th row.

The pull() function is used to extract the values of the "oPO4" variable from the filtered dataset and assigns them to the oPO4_reduced variable.

```{r}
res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

The below code utilizes a ggplot to portray a residual plot, which is a tool for assessing the assumptions of a linear regression model. The aes() function specifies that "oPO4" (predictor variable) should be plotted on the x-axis and "res" (containing residuals) should be plotted on the y-axis.

This plot allows to visually inspect whether the residuals are randomly distributed around zero, which is a key assumption for linear regression

```{r}
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

When the function fillPO4(x) is called, it will return a modified version of the input vector x where missing values have been replaced with values calculated using the formula 42.897 + 1.293 \* x. This function is useful for imputing missing values in numeric variables based on a specified formula or calculation.

```{r}
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
#if x is not NA, return 42.897+1.293*x 
```

In the below code the sapply() function applies the previously created function fillPO4 to the values in the oPO4 column of the selected rows(where "PO4" is missing)

```{r}
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

The function manyNAs() from the "DMwR2" package helps identify which rows have a significant amount of missing data. After running this code the "algae" dataset will be modified to exclude rows with a substantial amount of missing data.

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

The below code mainly utilizes the knnImputation() function which takes the weighted average of k (k=10) most similar samples to impute missing values. Then the dataset with missing values are reloaded again and the rows with many missing values are filtered out.\
Finally the knnImputation() function is used to impute missing values with the median of the k (k=10) most similar samples.

```{r}
algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


data(algae, package="DMwR2") #get data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #use the median of k most similar samples
```

Here, the getAnywhere() function displays the source code of the knnImputation() function.

```{r}
getAnywhere(knnImputation())
```

#### SCALING AND NORMALIZATION

**Scaling and normalization are data preprocessing techniques used to transform the features or variables in a dataset to make them suitable for machine learning algorithms.**

```{r}
library(dplyr)
library(palmerpenguins)
```

```{r}
data(penguins)
```

The below code selects the numeric columns from the penguins dataset and normalizes it using scale() function. Finally, the as.data.frame() function converts the normalized dataset into a data frame and a species column from the original dataset is added to it. This normalized data frame can be used for ML algorithms and analysis that require standardized intputs.

```{r}
# select only numeric columns
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# normalize numeric columns
penguins_norm <- scale(penguins_numeric)

# convert back to data frame and add species column
peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.
```

Summary of penguins dataset

```{r}
summary(penguins)
```

Summary of the peng.norm data frame

```{r}
summary(peng.norm)
```

The below code selects the max and min values from the penguins dataset excluding the species column (since it has characters in it) and also while avoiding NA values.

```{r}
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
```

```{r}
max
```

```{r}
min

```

The below code utilizes the lapply() function to apply a function to each column of the "penguins_numeric" dataset. The function (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) performs a min-max scaling on each column. The penguin_scaled dataset is binded with the species column using the cbind() function. FInally, the summary of the data frame is displayed.

```{r}
# min-max normalization
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

Here, the Boston dataset from the package "MASS" is loaded and the summary of the age column in the dataset is displayed.

```{r}
data(Boston, package="MASS")
summary(Boston$age)
```

In the below code, the binning function is used to bin the values in the "age" column into 5 equal width bins and add them to the new "newAge" column. Finally, the summary of the newAge column is displayed. Binning is a pre-processing technique that groups a series of numerical data into sets of bins.

```{r}
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") #create 5 bins and add new column newAge to Boston
summary(Boston$newAge)
```

This code is similar to the above code but labels are added to each bin created using labels argument in the binning() function.

```{r}
Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") #add labels

summary(Boston$newAge)
```

After running this code, we will have a modified "Boston" dataset with a new "newAge" column containing the binning results based on equal-depth bins. The table will show the distribution of observations across these bins, providing insights into how the "age" values are distributed in the dataset.

```{r}

install.packages("Hmisc")
library(Hmisc)
Boston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins and add new column newAge to Boston

table(Boston$newAge)
```

This code cuts the age column in the dataset into 5 bins where the function calculates the quantiles to create bins of equal depth. The factor() function is used to convert the binning results into a factor variable.

```{r}
Boston$newAge <- factor(cut2(Boston$age, g = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

The below code genrates a histogram of the age column. The break statement specifies the intervals for the histogram(here 0-101 with intervals of 10).

```{r}
hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 â€“ 101.
```

This code will generate a histogram plot that visualizes the distribution of ages in the "Boston" dataset. The x-axis will represent age ranges, and the y-axis will represent the frequency of observations in each age range. The bin width of 10 means that each bin will cover a range of 10 years.

```{r}
library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

Five numeric values are added to the vector "data".

```{r}
data <- c(10, 20, 30, 50, 100)
```

'max(abs(data)) calculates the maximum absolute value in the "data" vector. It takes the absolute value of all elements in the vector, finds the maximum among them, and returns that value.

nchar(max(abs(data))) counts the number of digits in the maximum absolute value in the "data" vector.

```{r}
(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters
```

The below code calculates a decimal scale for the "data" vector. It divides each element of the "data" vector by 10 raised to the power of "nDigits."

```{r}
(decimalScale = data / (10^nDigits))
```

The matrix() function turns the age vector into a matrix. The number of rows is calculated by dividing the length of the age vector by 5, creating a matrix with 3 rows.

byrow= TRUE indicates that the values should be filled into the matrix by rows.

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

"bin_means" vector will contain the mean (average) value of each bin from the "bins" matrix. Each element of "bin_means" corresponds to the mean of a specific bin or group of values from the original "age" data.

```{r}
(bin_means = apply(bins, 1, FUN = mean))
```

The provided code sets each row in the "bins" matrix to have the mean value of its respective bin.

```{r}
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

This code calculates the mean of each bin in the "bins" matrix and then rounds those mean values to two decimal places. The result is stored in a vector named "age_bin_mean_smoothed."

t(bins) transposes the "bins" matrix, swapping rows and columns, so that each column represents a bin.

as.vector() converts the transposed matrix into a vector, concatenating all the columns into a single vector.

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

#### **Variable correlations and dimensionality reduction**

**Variable correlations help us understand relationships between variables, while dimensionality reduction techniques allow us to simplify complex datasets by reducing the number of variables while retaining meaningful information.**

'test1 = chisq.test(racetable, correct=F)' This line performs a chi-squared test of independence using the "racetable" matrix. 'correct = F' specifies that no correction for continuity should be applied.

```{r}
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

The provided code creates a four-dimensional array named "seniors" with specific data values, dimensions ('dim = c(2, 2, 2, 2)'), and dimension names(dimnames=lost(".." etc)).

```{r}
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
seniors
```

seniors.tb \<- as.table(seniors): This line applies the as.table() function to the "seniors" array, converting it into a table format.

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

The 'seniors.df \<- as.data.frame(seniors.tb)' line applies the as.data.frame() function to the "seniors.tb" table, converting it into a data frame format. Converting the table to a data frame format allows you to perform various data manipulation and analysis tasks.

```{r}
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

Freq \~ (cigarette \* marijuana \* alcohol \* age) specifies the formula for the model.

It suggests that the response variable "Freq" is regressed on the interaction of three categorical predictor variables: "cigarette," "marijuana," "alcohol" and "age" .

data = seniors.df specifies the data frame from which the variables are taken.

family=poisson indicates that a Poisson distribution should be used for modeling the response variable.

The glm() function is used for fitting generalized linear models to a dataset.

```{r}
mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

```{r}
mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

The cbind function combines these two sets of values into a single matrix or data frame where the original data columns and the fitted values are side by side. This can be useful for comparing the actual data to the model's predictions and evaluating how well the model fits the data.

```{r}
cbind(mod.3$data, fitted(mod.3))
```

The code first removes rows with missing values and then calculates the correlation matrix for the remaining data. This can be useful for exploring the relationships between variables and identifying patterns in the data.

```{r}
library(tidyr) # data manipulation
penguins_numeric |> 
  drop_na() |>
  correlate()
```

Initially, the rows are removed from the dataset, and columns species, island, sex are removed.

pca: The princomp() function is used to perform Principal Component Analysis on the "pca.data" data frame. This function calculates the principal components of the numeric variables in the data frame.

loadings(pca): This line extracts the loadings of the principal components from the PCA result. Loadings represent the coefficients of the original variables in the linear combinations that make up the principal components.

By examining the loadings, you can gain insights into which original variables have the most influence on each principal component.

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

The head(pca\$scores) command is used to display the first few rows of the scores matrix obtained from the Principal Component Analysis (PCA).

```{r}
head(pca$scores) # pca result is a list, and the component scores are elements in the list
```

pca\$scores\[,1:3\]: This extracts the scores from the first three principal components (\[,1:3\]) obtained from the PCA analysis after the NA values are dropped.

Species = penguins_na\$species: This adds the "Species" column from the original "penguins_na" data frame to the reduced data frame.

The resulting "peng.reduced" data frame retains information about the first three principal components and the "Species" column, which can be useful for further analysis or visualization of the reduced-dimensional data.

```{r}
penguins_na <- penguins |> 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

```{r}
install.packages("wavelets")
library(wavelets)
```

wt \<- dwt(x, filter = "haar", n.levels = 3): The dwt function performs the discrete wavelet transform (DWT) on the vector x.

filter = "haar": Specifies the Haar wavelet as the wavelet filter to be used for the transformation.

n.levels = 3: Indicates that three levels of decomposition should be performed.

The result, stored in the wt variable, will include information about how the signal x is decomposed into different levels of approximation and detail coefficients.

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3) #with 8-element vector, 3 level is the max.
wt
```

The idwt function is typically used to perform the inverse discrete wavelet transform (IDWT) to reconstruct the original signal from its wavelet coefficients.

```{r}
idwt(wt)
```

The dwt function performs the discrete wavelet transform (DWT) on the vector x.

filter = wt.filter(c(0.5, -0.5)): Specifies a custom wavelet filter for the transformation. In this case, the filter is defined as \[0.5, -0.5\].

n.levels = 3: Indicates that three levels of decomposition should be performed.

```{r}
xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

```{r}
idwt(xt)
```

#### SAMPLING

**Sampling is a statistical process of selecting a subset of individuals or items from a larger population or dataset for the purpose of making inferences or observations about the population.**

set.seed(1) sets the seed for the random number generator.

There is a vector called "age" with 12 age values.

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

The sample(age, 5) command selects a random sample of 5 values from the "age" vector. Since seed is set to using set.seed(1), the random sampling will be reproducible.

```{r}
sample(age, 5)
```

Sample() function selects 5 random values from the "age" vector with replacement. The replace = TRUE argument allows the same value to be selected more than once in the sample.

```{r}
sample(age, 5, replace = TRUE)
```

By setting the seed and then summarizing the dataset, we ensure that the summary statistics are based on the same random operations each time you run the code.

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

This code first groups the "algae" dataset by the "season" variable using the group_by function. Then, it selects a random sample of 25% of the data within each season group using the sample_frac function.

This can be useful for exploring and analyzing a representative portion of the data within each season category.

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

The first line defines a vector called "age" containing a list of ages. Then the kmeans function performs k-means clustering on the "age" vector, where 3 is specified as the number of clusters to form.

s\$cluster: This extracts the cluster assignments from the result of the k-means clustering and stores them in the "cluster" vector.

The result is a vector called "s\$cluster" that contains cluster assignments for each age in the "age" vector.

```{r}

age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

ageframe\$condition \<- s\$cluster: This adds a new column called "condition" to the "ageframe" data frame. The values in this column are assigned based on the cluster labels obtained from the k-means clustering performed earlier using the "s\$cluster" vector.

cluster(ageframe, clustername = "condition", size = 2): This line uses the cluster function to select 2 clusters out of the 3 based on the "condition" column.

The result is a subset of the data frame "ageframe," containing data points belonging to two of the three clusters obtained from the k-means clustering.

```{r}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

Unable to recreate the **Handling Text Datasets** since I'm getting error "Error in DirSource("Documents"): empty directory"
